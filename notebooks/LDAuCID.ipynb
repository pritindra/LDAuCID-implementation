{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgbNit_wT4it",
        "outputId": "03cf04a3-85ca-4fd7-f0e6-770770937db1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  287M  100  287M    0     0  65.8M      0  0:00:04  0:00:04 --:--:-- 65.8M\n",
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "   creating: dataset/part_two_dataset/\n",
            "  inflating: dataset/.DS_Store       \n",
            "  inflating: __MACOSX/dataset/._.DS_Store  \n",
            "  inflating: dataset/README.md       \n",
            "  inflating: __MACOSX/dataset/._README.md  \n",
            "   creating: dataset/part_one_dataset/\n",
            "  inflating: dataset/part_two_dataset/.DS_Store  \n",
            "  inflating: __MACOSX/dataset/part_two_dataset/._.DS_Store  \n",
            "   creating: dataset/part_two_dataset/train_data/\n",
            "   creating: dataset/part_two_dataset/eval_data/\n",
            "  inflating: dataset/part_one_dataset/.DS_Store  \n",
            "  inflating: __MACOSX/dataset/part_one_dataset/._.DS_Store  \n",
            "   creating: dataset/part_one_dataset/train_data/\n",
            "   creating: dataset/part_one_dataset/eval_data/\n",
            "  inflating: dataset/part_two_dataset/train_data/6_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/8_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/5_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/3_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/10_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/1_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/2_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/9_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/4_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/7_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/8_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/9_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/10_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/5_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/4_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/1_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/7_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/2_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/6_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/3_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/6_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/8_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/5_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/3_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/10_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/1_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/2_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/9_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/4_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/7_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/8_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/9_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/10_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/5_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/4_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/1_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/7_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/2_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/6_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/3_eval_data.tar.pth  \n"
          ]
        }
      ],
      "source": [
        "!rm -rf *\n",
        "!curl 'https://drive.usercontent.google.com/download?id=19DPObbiUbzGFEbCoPAyixrv_JT5QCQXE&export=download&authuser=0&confirm=t&uuid=7869aa1b-8a2e-4169-a9ee-f1f2d7311078&at=AENtkXYJgijttsPeTTrrX2CrUGaz%3A1730284122447' > dataset.zip\n",
        "!unzip dataset.zip\n",
        "!rm -rf dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os, tarfile\n",
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from collections import defaultdict\n",
        "\n",
        "# 📁 Define data path\n",
        "DATASET_PATH = '/content/dataset/part_one_dataset/train_data'"
      ],
      "metadata": {
        "id": "HxMbRq54UUcW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(index):\n",
        "    file_path = os.path.join(DATASET_PATH, f'{index}_train_data.tar.pth')\n",
        "\n",
        "    # Load ignoring strict weight-only enforcement (if PyTorch >= 2.1.0)\n",
        "    try:\n",
        "        data_dict = torch.load(file_path, map_location='cpu', weights_only=False)\n",
        "    except TypeError:\n",
        "        # fallback for older torch versions without weights_only\n",
        "        data_dict = torch.load(file_path, map_location='cpu')\n",
        "\n",
        "    return data_dict.get('data', None), data_dict.get('labels', None)"
      ],
      "metadata": {
        "id": "zEXh82BgW_Tj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim=1024, latent_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, latent_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, latent_dim=128, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(latent_dim, num_classes)\n",
        "    def forward(self, z):\n",
        "        return self.fc(z)"
      ],
      "metadata": {
        "id": "FwHYIrd4XEml"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_gmm(latents, labels, num_classes=10):\n",
        "    gmm_dict = {}\n",
        "    for cls in range(num_classes):\n",
        "        cls_latents = latents[labels == cls]\n",
        "        gmm = GaussianMixture(n_components=1, covariance_type='full')\n",
        "        gmm.fit(cls_latents)\n",
        "        gmm_dict[cls] = gmm\n",
        "    return gmm_dict"
      ],
      "metadata": {
        "id": "skP2lewRbEYq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliced_wasserstein_distance(P, Q, num_projections=50):\n",
        "    P, Q = P.detach().cpu().numpy(), Q.detach().cpu().numpy()\n",
        "    d = P.shape[1]\n",
        "    swd = 0.0\n",
        "    for _ in range(num_projections):\n",
        "        proj = np.random.normal(size=(d,))\n",
        "        proj /= np.linalg.norm(proj)\n",
        "        proj_P = np.dot(P, proj)\n",
        "        proj_Q = np.dot(Q, proj)\n",
        "        proj_P.sort(), proj_Q.sort()\n",
        "        swd += np.mean(np.abs(proj_P - proj_Q))\n",
        "    return swd / num_projections"
      ],
      "metadata": {
        "id": "vKkL4etzbGri"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pseudo_data(gmm_dict, classifier, num_samples=100, threshold=0.7):\n",
        "    pseudo_X, pseudo_Y = [], []\n",
        "    classifier.eval()\n",
        "    for cls, gmm in gmm_dict.items():\n",
        "        z = gmm.sample(num_samples)[0]\n",
        "        z_tensor = torch.tensor(z, dtype=torch.float32)\n",
        "        with torch.no_grad():\n",
        "            logits = classifier(z_tensor)\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            confs, preds = torch.max(probs, dim=1)\n",
        "            mask = confs > threshold\n",
        "            pseudo_X.append(z_tensor[mask])\n",
        "            pseudo_Y.append(preds[mask])\n",
        "    return torch.cat(pseudo_X), torch.cat(pseudo_Y)"
      ],
      "metadata": {
        "id": "GdiLGnkHbJ5Z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_on_domain(encoder, classifier, data, gmm_dict, replay_buffer, epochs=5):\n",
        "    encoder.train(), classifier.train()\n",
        "    optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass for target domain\n",
        "        x_t = torch.tensor(data, dtype=torch.float32)\n",
        "        z_t = encoder(x_t)\n",
        "\n",
        "        # SWD between current and internal distribution\n",
        "        z_gmm, _ = generate_pseudo_data(gmm_dict, classifier, num_samples=100)\n",
        "        swd = sliced_wasserstein_distance(z_t, z_gmm)\n",
        "\n",
        "        loss = torch.tensor(swd, requires_grad=True)\n",
        "\n",
        "        # Experience replay if available\n",
        "        if replay_buffer:\n",
        "            x_replay, y_replay = zip(*replay_buffer)\n",
        "            x_replay = torch.stack(x_replay)\n",
        "            y_replay = torch.tensor(y_replay)\n",
        "            z_replay = encoder(x_replay)\n",
        "            pred_replay = classifier(z_replay)\n",
        "            loss += nn.CrossEntropyLoss()(pred_replay, y_replay)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "rDyJrw3ybMgf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "encoder = Encoder(input_dim=1024, latent_dim=64)\n",
        "classifier = Classifier(latent_dim=64, num_classes=10)\n",
        "\n",
        "replay_buffer = []\n",
        "all_latents, all_labels, domain_tags = [], [], []\n",
        "\n",
        "# Initial training on Domain 0 (labeled)\n",
        "src_data, src_labels = load_dataset(0)\n",
        "x = torch.tensor(src_data, dtype=torch.float32)\n",
        "y = torch.tensor(src_labels)\n",
        "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
        "for _ in range(10):\n",
        "    encoder.train(), classifier.train()\n",
        "    z = encoder(x)\n",
        "    logits = classifier(z)\n",
        "    loss = nn.CrossEntropyLoss()(logits, y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Fit GMM on latent space\n",
        "with torch.no_grad():\n",
        "    z = encoder(x)\n",
        "    gmm_dict = fit_gmm(z.numpy(), src_labels)\n",
        "\n",
        "# Add representative samples to replay buffer (Mean-of-Features)\n",
        "for cls in range(10):\n",
        "    cls_mask = (y == cls)\n",
        "    z_cls = z[cls_mask]\n",
        "    x_cls = x[cls_mask]\n",
        "    dists = torch.norm(z_cls - z_cls.mean(0), dim=1)\n",
        "    topk = torch.topk(-dists, k=min(5, len(dists))).indices\n",
        "    for i in topk:\n",
        "        replay_buffer.append((x_cls[i], int(cls)))\n",
        "\n",
        "# UDA for domains 1–9\n",
        "for d in range(1, 10):\n",
        "    print(f\"\\n➡️ Adapting to domain {d}\")\n",
        "    data, _ = load_dataset(d)\n",
        "    train_on_domain(encoder, classifier, data, gmm_dict, replay_buffer, epochs=5)\n",
        "\n",
        "    # Update GMM\n",
        "    with torch.no_grad():\n",
        "        x_t = torch.tensor(data, dtype=torch.float32)\n",
        "        z_t = encoder(x_t)\n",
        "        pseudo_labels = torch.argmax(classifier(z_t), dim=1)\n",
        "        gmm_dict = fit_gmm(z_t.numpy(), pseudo_labels.numpy())\n",
        "\n",
        "        # Store for UMAP\n",
        "        all_latents.append(z_t)\n",
        "        all_labels.append(pseudo_labels)\n",
        "        domain_tags += [d] * len(pseudo_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "1vjHCxG1bTGj",
        "outputId": "f65e4c00-d275-4e84-f817-39d988fafb56"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/dataset/part_one_dataset/train_data/0_train_data.tar.pth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-1140780146>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Initial training on Domain 0 (labeled)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msrc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-1032116315>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Load ignoring strict weight-only enforcement (if PyTorch >= 2.1.0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# fallback for older torch versions without weights_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/dataset/part_one_dataset/train_data/0_train_data.tar.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DYw3iZC2bfCa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}