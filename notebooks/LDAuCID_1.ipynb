{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgbNit_wT4it",
        "outputId": "b255b763-6cd1-48fa-dc04-50f0ca0409e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  287M  100  287M    0     0  48.8M      0  0:00:05  0:00:05 --:--:-- 70.0M\n",
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "   creating: dataset/part_two_dataset/\n",
            "  inflating: dataset/.DS_Store       \n",
            "  inflating: __MACOSX/dataset/._.DS_Store  \n",
            "  inflating: dataset/README.md       \n",
            "  inflating: __MACOSX/dataset/._README.md  \n",
            "   creating: dataset/part_one_dataset/\n",
            "  inflating: dataset/part_two_dataset/.DS_Store  \n",
            "  inflating: __MACOSX/dataset/part_two_dataset/._.DS_Store  \n",
            "   creating: dataset/part_two_dataset/train_data/\n",
            "   creating: dataset/part_two_dataset/eval_data/\n",
            "  inflating: dataset/part_one_dataset/.DS_Store  \n",
            "  inflating: __MACOSX/dataset/part_one_dataset/._.DS_Store  \n",
            "   creating: dataset/part_one_dataset/train_data/\n",
            "   creating: dataset/part_one_dataset/eval_data/\n",
            "  inflating: dataset/part_two_dataset/train_data/6_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/8_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/5_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/3_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/10_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/1_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/2_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/9_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/4_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/train_data/7_train_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/8_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/9_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/10_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/5_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/4_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/1_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/7_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/2_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/6_eval_data.tar.pth  \n",
            "  inflating: dataset/part_two_dataset/eval_data/3_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/6_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/8_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/5_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/3_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/10_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/1_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/2_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/9_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/4_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/train_data/7_train_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/8_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/9_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/10_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/5_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/4_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/1_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/7_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/2_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/6_eval_data.tar.pth  \n",
            "  inflating: dataset/part_one_dataset/eval_data/3_eval_data.tar.pth  \n"
          ]
        }
      ],
      "source": [
        "!rm -rf *\n",
        "!curl 'https://drive.usercontent.google.com/download?id=19DPObbiUbzGFEbCoPAyixrv_JT5QCQXE&export=download&authuser=0&confirm=t&uuid=7869aa1b-8a2e-4169-a9ee-f1f2d7311078&at=AENtkXYJgijttsPeTTrrX2CrUGaz%3A1730284122447' > dataset.zip\n",
        "!unzip dataset.zip\n",
        "!rm -rf dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "New implementation"
      ],
      "metadata": {
        "id": "4XftESWQytyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "class CNNEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim=64):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),  # (B, 32, 32, 32)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                                       # (B, 32, 16, 16)\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),           # (B, 64, 16, 16)\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),                                       # (B, 64, 8, 8)\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),          # (B, 128, 8, 8)\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1, 1))                           # (B, 128, 1, 1)\n",
        "        )\n",
        "        self.fc = nn.Linear(128, latent_dim)                        # Final latent vector\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)  # (B, 128, 1, 1)\n",
        "        x = x.view(x.size(0), -1)  # Flatten to (B, 128)\n",
        "        return self.fc(x)          # (B, latent_dim)\n",
        "\n",
        "# Classification head: maps latent to class logits\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc = nn.Linear(latent_dim, num_classes)\n",
        "    def forward(self, z):\n",
        "        return self.fc(z)\n",
        "\n",
        "# Utility: compute class prototypes (mean of features) given features and labels\n",
        "def compute_prototypes(features, labels, num_classes):\n",
        "    prototypes = []\n",
        "    for c in range(num_classes):\n",
        "        class_feats = features[labels == c]\n",
        "        if len(class_feats) > 0:\n",
        "            prototypes.append(class_feats.mean(dim=0))\n",
        "        else:\n",
        "            prototypes.append(torch.zeros(features.shape[1]))\n",
        "    return torch.stack(prototypes)  # shape: (num_classes, latent_dim)\n",
        "\n",
        "# Utility: compute Euclidean distance between features and prototypes\n",
        "def l2_distances(features, prototypes):\n",
        "    # features: (batch, dim), prototypes: (num_classes, dim)\n",
        "    # Returns (batch, num_classes) distances\n",
        "    # ||f - p||^2 = ||f||^2 + ||p||^2 - 2 fÂ·p\n",
        "    f_sq = (features**2).sum(dim=1, keepdim=True)\n",
        "    p_sq = (prototypes**2).sum(dim=1)\n",
        "    dist = f_sq + p_sq.unsqueeze(0) - 2 * features @ prototypes.t()\n",
        "    return dist\n",
        "\n",
        "# Compute Sliced Wasserstein Distance (SWD) between two sets of vectors\n",
        "def sliced_wasserstein_distance(X, Y, num_projections=50, device='cpu'):\n",
        "    # X, Y: tensors of shape (n_samples, feature_dim)\n",
        "    # Sample random directions on the unit sphere\n",
        "    d = X.size(1)\n",
        "    swd = 0.0\n",
        "    for _ in range(num_projections):\n",
        "        # Draw a random direction vector\n",
        "        theta = torch.randn(d, device=device)\n",
        "        theta = theta / theta.norm()\n",
        "        proj_X = X @ theta  # (n,)\n",
        "        proj_Y = Y @ theta\n",
        "        # Sort projections\n",
        "        proj_X, _ = torch.sort(proj_X)\n",
        "        proj_Y, _ = torch.sort(proj_Y)\n",
        "        # 1D Wasserstein: L2 distance of sorted projections\n",
        "        swd += torch.mean((proj_X - proj_Y)**2)\n",
        "    return swd / num_projections\n",
        "\n",
        "# GMM internal distribution: fit per-class Gaussian parameters on latent features\n",
        "class LatentGMM:\n",
        "    def __init__(self, num_classes, latent_dim):\n",
        "        self.num_classes = num_classes\n",
        "        self.means = torch.zeros(num_classes, latent_dim)\n",
        "        self.covariances = torch.zeros(num_classes, latent_dim, latent_dim)\n",
        "        self.weights = torch.zeros(num_classes)\n",
        "    def fit(self, features, labels):\n",
        "        N = len(labels)\n",
        "        for c in range(self.num_classes):\n",
        "            idx = (labels == c).nonzero(as_tuple=True)[0]\n",
        "            Nc = len(idx)\n",
        "            if Nc > 0:\n",
        "                feats_c = features[idx]\n",
        "                self.means[c] = feats_c.mean(dim=0)\n",
        "                # covariance (latent_dim x latent_dim)\n",
        "                centered = feats_c - self.means[c]\n",
        "                cov = centered.t() @ centered / (Nc + 1e-6)\n",
        "                self.covariances[c] = cov + 1e-6 * torch.eye(centered.size(1))\n",
        "                self.weights[c] = Nc / N\n",
        "            else:\n",
        "                self.means[c].zero_()\n",
        "                self.covariances[c] = torch.eye(features.size(1))\n",
        "                self.weights[c] = 1.0 / self.num_classes\n",
        "    def sample(self, n_samples):\n",
        "        # Sample from the GMM: choose component according to weights, then Gaussian\n",
        "        if self.num_classes == 1:\n",
        "            comp = torch.zeros(n_samples, dtype=torch.long)\n",
        "        else:\n",
        "            comp = torch.multinomial(self.weights, num_samples=n_samples, replacement=True)\n",
        "        Z = torch.zeros(n_samples, self.means.size(1))\n",
        "        for i, c in enumerate(comp):\n",
        "            mean = self.means[c]\n",
        "            cov = self.covariances[c]\n",
        "            # Sample from multivariate normal (using Cholesky)\n",
        "            L = torch.cholesky(cov + 1e-6 * torch.eye(cov.size(0)))\n",
        "            z = mean + L @ torch.randn(cov.size(0))\n",
        "            Z[i] = z\n",
        "        return Z\n"
      ],
      "metadata": {
        "id": "DYw3iZC2bfCa"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Custom Dataset class for loading train and eval domains\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_path, transform=None, labeled=True):\n",
        "        self.data_dict = torch.load(file_path, map_location='cpu', weights_only=False)\n",
        "        self.data = self.data_dict['data']  # tensor of shape (N, C, H, W) or (N, F)\n",
        "        self.transform = transform\n",
        "        self.labeled = labeled\n",
        "        if labeled:\n",
        "            self.targets = self.data_dict.get('targets', None)  # should be tensor of shape (N,)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data[idx]\n",
        "        if isinstance(img, np.ndarray):\n",
        "            img = torch.from_numpy(img)\n",
        "        img = img.permute(2,0,1).float() / 255.0\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        if self.labeled:\n",
        "            label = self.targets[idx]\n",
        "            return img, label\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "# Utility function to load dataset object\n",
        "def load_dataset(domain_idx, train=True, transform=None):\n",
        "    base = \"dataset/part_one_dataset\"\n",
        "    subdir = \"train_data\" if train else \"eval_data\"\n",
        "    filename = f\"{domain_idx}_{'train' if train else 'eval'}_data.tar.pth\"\n",
        "    path = os.path.join(base, subdir, filename)\n",
        "    labeled = train and domain_idx == 1 or not train  # Only domain 1 is labeled in training, all eval are labeled\n",
        "    return CustomDataset(path, transform=transform, labeled=labeled)\n"
      ],
      "metadata": {
        "id": "4D0HJQgNyv1L"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Assuming encoder and classifier are already trained on D1 and available\n",
        "# Also assume latent_D1 and y1_train are available as torch.Tensors\n",
        "\n",
        "# Define GMM class from previous code (reused)\n",
        "class LatentGMM:\n",
        "    def __init__(self, num_classes, latent_dim):\n",
        "        self.num_classes = num_classes\n",
        "        self.latent_dim = latent_dim\n",
        "        self.means = torch.zeros(num_classes, latent_dim)\n",
        "        self.covariances = torch.stack([torch.eye(latent_dim) for _ in range(num_classes)])\n",
        "        self.weights = torch.ones(num_classes) / num_classes\n",
        "\n",
        "    def fit(self, features, labels):\n",
        "        N = len(labels)\n",
        "        for c in range(self.num_classes):\n",
        "            idx = (labels == c).nonzero(as_tuple=True)[0]\n",
        "            if len(idx) > 0:\n",
        "                feats_c = features[idx]\n",
        "                mean = feats_c.mean(dim=0)\n",
        "                cov = (feats_c - mean).T @ (feats_c - mean) / (len(idx) + 1e-6)\n",
        "                self.means[c] = mean\n",
        "                self.covariances[c] = cov + 1e-6 * torch.eye(self.latent_dim)\n",
        "                self.weights[c] = len(idx) / N\n",
        "            else:\n",
        "                self.means[c] = torch.zeros(self.latent_dim)\n",
        "                self.covariances[c] = torch.eye(self.latent_dim)\n",
        "                self.weights[c] = 1.0 / self.num_classes\n",
        "        self.weights /= self.weights.sum()\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        components = torch.multinomial(self.weights, num_samples=num_samples, replacement=True)\n",
        "        Z = torch.zeros((num_samples, self.latent_dim))\n",
        "        for i, c in enumerate(components):\n",
        "            mean = self.means[c]\n",
        "            cov = self.covariances[c]\n",
        "            L = torch.linalg.cholesky(cov)\n",
        "            z = mean + L @ torch.randn(self.latent_dim)\n",
        "            Z[i] = z\n",
        "        return Z\n",
        "\n",
        "def generate_pseudo_data(gmm, classifier, tau=0.8, num_samples=1000):\n",
        "    \"\"\"\n",
        "    Draw latent samples from the internal GMM and assign pseudo-labels\n",
        "    using the classifier. Return confident latent vectors and their labels.\n",
        "    \"\"\"\n",
        "    Z = gmm.sample(num_samples)  # (num_samples, latent_dim)\n",
        "    with torch.no_grad():\n",
        "        logits = classifier(Z)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        conf, pred = torch.max(probs, dim=1)\n",
        "        mask = conf > tau\n",
        "    return Z[mask], pred[mask]\n"
      ],
      "metadata": {
        "id": "8fOsZaR7z1b9"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWikUpl188LY",
        "outputId": "6b558087-add5-4003-df7f-225a894ea335"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 5\n",
        "batch_size = 64\n",
        "lambda_swd = 1.0\n",
        "tau = 0.8\n",
        "memory_size_per_class = 20\n",
        "\n",
        "# Initialize encoder and classifier\n",
        "input_dim = 1024  # Set this based on your data\n",
        "latent_dim = 64\n",
        "num_classes = 10\n",
        "\n",
        "encoder = CNNEncoder(latent_dim=latent_dim)\n",
        "classifier = Classifier(latent_dim=latent_dim, num_classes=num_classes)\n",
        "optimizer = optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=1e-3)\n",
        "\n",
        "# === Initial training on D1 === #\n",
        "d1_dataset = load_dataset(1, train=True)\n",
        "d1_loader = DataLoader(d1_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "    for x, y in d1_loader:\n",
        "        z = encoder(x)\n",
        "        logits = classifier(z)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Extract D1 features and labels for GMM and memory\n",
        "encoder.eval()\n",
        "with torch.no_grad():\n",
        "    feats, labels = [], []\n",
        "    for x, y in d1_loader:\n",
        "        feats.append(encoder(x))\n",
        "        labels.append(y)\n",
        "feats = torch.cat(feats)\n",
        "labels = torch.cat(labels)\n",
        "\n",
        "# Initialize prototypes and memory using MoF\n",
        "prototypes = compute_prototypes(feats, labels, num_classes)\n",
        "memory_X, memory_y = [], []\n",
        "for c in range(num_classes):\n",
        "    idx = (labels == c).nonzero(as_tuple=True)[0]\n",
        "    if len(idx) > 0:\n",
        "        dists = torch.norm(feats[idx] - prototypes[c], dim=1)\n",
        "        top_idx = idx[torch.argsort(dists)[:memory_size_per_class]]\n",
        "        imgs = [\n",
        "          torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
        "          for img in dt_dataset.data[top_idx]\n",
        "        ]\n",
        "        memory_X.append(torch.stack(imgs))\n",
        "        memory_y.append(labels[top_idx])\n",
        "memory_X = torch.cat(memory_X)\n",
        "memory_y = torch.cat(memory_y)\n",
        "\n",
        "# Initialize GMM\n",
        "gmm = LatentGMM(num_classes=num_classes, latent_dim=latent_dim)\n",
        "gmm.fit(feats, labels)\n",
        "\n",
        "# === Continual adaptation from domains 2 to 10 === #\n",
        "for t in range(2, 11):\n",
        "    print(f\"\\nð Adapting to domain {t}\")\n",
        "    # Load current target domain\n",
        "    dt_dataset = load_dataset(t, train=True)\n",
        "    dt_loader = DataLoader(dt_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Generate pseudo-latent samples\n",
        "    Z_pseudo, y_pseudo = generate_pseudo_data(gmm, classifier, tau=tau, num_samples=1000)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for x_t in dt_loader:\n",
        "            if isinstance(x_t, tuple):  # skip labels if they exist\n",
        "                x_t = x_t[0]\n",
        "\n",
        "            # Get memory & pseudo batches\n",
        "            mem_idx = random.sample(range(len(memory_X)), min(batch_size, len(memory_X)))\n",
        "            pseudo_idx = random.sample(range(len(Z_pseudo)), min(batch_size, len(Z_pseudo)))\n",
        "            X_mem = memory_X[mem_idx]; y_mem = memory_y[mem_idx]\n",
        "            Z_mem = Z_pseudo[pseudo_idx]; y_pseudo_batch = y_pseudo[pseudo_idx]\n",
        "            if Z_mem.size(0) == 0 or feats_t.size(0) == 0:\n",
        "                continue  # skip this batch\n",
        "\n",
        "            # Forward passes\n",
        "            feats_t = encoder(x_t)\n",
        "            feats_mem = encoder(X_mem)\n",
        "            logits_mem = classifier(feats_mem)\n",
        "            logits_pseudo = classifier(Z_mem)\n",
        "\n",
        "            # Losses\n",
        "            loss_ce = F.cross_entropy(logits_mem, y_mem) + F.cross_entropy(logits_pseudo, y_pseudo_batch)\n",
        "            swd = sliced_wasserstein_distance(feats_t, Z_mem)\n",
        "            loss = loss_ce + lambda_swd * swd\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # === Update memory buffer === #\n",
        "    encoder.eval()\n",
        "    all_feats, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for x_t in DataLoader(dt_dataset, batch_size=batch_size):\n",
        "            if isinstance(x_t, tuple):  # if data includes labels\n",
        "                x_t = x_t[0]\n",
        "            feats = encoder(x_t)\n",
        "            logits = classifier(feats)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            all_feats.append(feats)\n",
        "            all_preds.append(preds)\n",
        "    all_feats = torch.cat(all_feats)\n",
        "    all_preds = torch.cat(all_preds)\n",
        "\n",
        "    prototypes = compute_prototypes(encoder(memory_X), memory_y, num_classes)\n",
        "    for c in range(num_classes):\n",
        "        idx_c = (all_preds == c).nonzero(as_tuple=True)[0]\n",
        "        if len(idx_c) > 0:\n",
        "            dists = torch.norm(all_feats[idx_c] - prototypes[c], dim=1)\n",
        "            top_idx = idx_c[torch.argsort(dists)[:memory_size_per_class]]\n",
        "\n",
        "            new_imgs = [\n",
        "                torch.from_numpy(img).permute(2, 0, 1).float() / 255.0\n",
        "                for img in dt_dataset.data[top_idx]\n",
        "            ]\n",
        "            new_imgs_tensor = torch.stack(new_imgs)\n",
        "\n",
        "            memory_X = torch.cat([memory_X, new_imgs_tensor])\n",
        "            memory_y = torch.cat([memory_y, torch.full((len(top_idx),), c, dtype=torch.long)])\n",
        "\n",
        "    # Refit GMM on current memory buffer\n",
        "    with torch.no_grad():\n",
        "        feats_mem_total = encoder(memory_X)\n",
        "    gmm.fit(feats_mem_total, memory_y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSLGXabd0LTj",
        "outputId": "4ca9c891-0135-4b6f-870a-8511da578720"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ð Adapting to domain 2\n",
            "\n",
            "ð Adapting to domain 3\n",
            "\n",
            "ð Adapting to domain 4\n",
            "\n",
            "ð Adapting to domain 5\n",
            "\n",
            "ð Adapting to domain 6\n",
            "\n",
            "ð Adapting to domain 7\n",
            "\n",
            "ð Adapting to domain 8\n",
            "\n",
            "ð Adapting to domain 9\n",
            "\n",
            "ð Adapting to domain 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Prepare accuracy matrix\n",
        "accuracies = torch.zeros(10, 10)  # rows: after training domain i, cols: test on eval domain j\n",
        "\n",
        "# Device (CPU or CUDA)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Send models to device\n",
        "encoder.to(device)\n",
        "classifier.to(device)\n",
        "\n",
        "# Evaluation function using DataLoader\n",
        "def evaluate(encoder, classifier, dataset):\n",
        "    encoder.eval()\n",
        "    classifier.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=64)\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            z = encoder(x)\n",
        "            logits = classifier(z)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.append(preds)\n",
        "            all_labels.append(y)\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    return (all_preds == all_labels).float().mean().item()\n",
        "\n",
        "# Evaluate after domain 1 training\n",
        "for j in range(1, 11):\n",
        "    eval_dataset = load_dataset(j, train=False)  # labeled eval data\n",
        "    acc = evaluate(encoder, classifier, eval_dataset)\n",
        "    accuracies[0, j - 1] = acc\n",
        "    print(f\"Eval after D1 â Eval D{j}: {acc:.2%}\")\n",
        "\n",
        "# Evaluate after domains 2â10 adaptation\n",
        "for i in range(2, 11):\n",
        "    print(f\"\\nð Evaluating after training up to domain D{i}\")\n",
        "    for j in range(1, 11):\n",
        "        eval_dataset = load_dataset(j, train=False)\n",
        "        acc = evaluate(encoder, classifier, eval_dataset)\n",
        "        accuracies[i - 1, j - 1] = acc\n",
        "        print(f\"Eval after D{i} â Eval D{j}: {acc:.2%}\")\n",
        "\n",
        "# Print full matrix\n",
        "print(\"\\nð Accuracy matrix (rows: after D1âD10, cols: Eval D1âD10):\")\n",
        "print((accuracies * 100).round(decimals=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 681
        },
        "id": "fWsfE9qb0P5Z",
        "outputId": "ae50b2cc-830f-49ad-8213-f94159e5a39a"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval after D1 â Eval D1: 18.04%\n",
            "Eval after D1 â Eval D2: 17.72%\n",
            "Eval after D1 â Eval D3: 16.88%\n",
            "Eval after D1 â Eval D4: 18.28%\n",
            "Eval after D1 â Eval D5: 17.08%\n",
            "Eval after D1 â Eval D6: 18.04%\n",
            "Eval after D1 â Eval D7: 17.72%\n",
            "Eval after D1 â Eval D8: 16.80%\n",
            "Eval after D1 â Eval D9: 17.28%\n",
            "Eval after D1 â Eval D10: 16.16%\n",
            "\n",
            "ð Evaluating after training up to domain D2\n",
            "Eval after D2 â Eval D1: 18.04%\n",
            "Eval after D2 â Eval D2: 17.72%\n",
            "Eval after D2 â Eval D3: 16.88%\n",
            "Eval after D2 â Eval D4: 18.28%\n",
            "Eval after D2 â Eval D5: 17.08%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-60-3649072329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0meval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Eval after D{i} â Eval D{j}: {acc:.2%}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-60-3649072329.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, classifier, dataset)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-46-3590404632.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (B, 128, 1, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten to (B, 128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# (B, latent_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         return F.max_pool2d(\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uf7mKCe30Tep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}